# lightGBM
可用于排序、分类、回归
XGBOOST不足  训练耗时长，内存占用打
lightGBM 速度提升了10倍，占用内存降低了3倍左右。
采用leaf-wise策略分裂叶子节点，一般的提升算法分列树采用的是深度方向或者水平方向

传统的机器学习一般不能支持直接输入类别特征，需要先转化成多维的0-1特征，这样无论在空间上还是时间上效率都不高。
## 支持类别特征 LightGBM通过更改决策树算法的决策规则，直接原生支持类别特征，不需要转化，提高了近8倍的速度。
## 支持并行学习：特征并行 数据并行

lightGBM的特点：基于 Histogram 的决策树算法 带深度限制的 Leaf-wise 的叶子生长策略 直方图做差加速 直接支持类别特征(Categorical Feature)
  Histogram：连续特征：离散化
             分类特征：bin
             算法： For all Leaf p in T(X):
                       For all f in X.Features:
                           构建直方图H
                       For i in (0,num of row):
                           从直方图中找到最佳分裂点
                           For i in (len(H)):
                               
Cache 命中率优化 基于直方图的稀疏特征优化 多线程优化
lightGBM速度快：原因
内存占用率低：原因
互斥特征绑定（Exclusive Feature Bundling）
基于梯度的单边采样（GOSS）

# XGBOOST
损失函数加入了正则化项
XGBOOST是很多CART回归树的集成
XGBOOST的算法思想：1.对所有特征都按照特征的数值进行预排序
2.遍历分割点，找到特征上的最好分割点
3.将数据分裂成左右子节点

## 优点 
1.利用二阶梯度划分节点，精度高
2.利用局部近似算法对分裂节点的贪心算法优化，速度快
3.损失函数加入了L1/L2,提高了模型的鲁棒性
4.提供并行计算能力，主要是在树节点求不同的候选的分裂点的Gain Infomation
5.Tree Shrinkage，column subsampling等不同的处理细节。
## 缺点
1.需要pre-sorted，这个会耗掉很多的内存空间
2.数据分割点上，由于XGB对不同的数据特征使用pre-sorted算法而不同特征其排序顺序是不同的，所以分裂时需要对每个特征单独做依次分割，遍历次数为#data * #features来将数据分裂到左右子节点上。
3.尽管使用了局部近似计算，但是处理粒度还是太细了
4.由于pre-sorted处理数据，在寻找特征分裂点时（level-wise），会产生大量的cache随机访问。

# GBDT
GBDT 分类和回归的基学习器都是CART回归树，因为是残差拟合的。
分类问题：指数损失函数、对数损失函数
回归问题：均方差损失函数、绝对损失函数。
        分位数、Huber损失函数，可以增加回归问题的健壮性，可以减少异常点对损失函数的影响。

