## 模型融合
Stacking是堆叠使用基础分类器的预测作为对二级模型训练的输入。如果在训练集上训练、预测，会造成标签。因此使用K-fold交叉验证，将K个模型的验证集的预测结果
拼起来，作为下一层学习器的输入。
Bagging:随机森林，方差小，更稳定
Boosting:GBDT，偏差小，预测能力更强。

## 随机森林
弱模型：CART  
随机：构造的模型有一定的随机性--随机抽取样本，随机抽取特征。  
优点：不易陷入过拟合，具有很好的抗噪能力。随机性保证了各子模型间的多样性，子模型间差异大，模型融合起来的效果会越好。  
每一棵决策树模型的训练是通过 自助采样法(Boostrap抽样)抽出来的。未被抽出的样本作为这个子模型的测试集。  
减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一
的一个参数。  
OOB估计：1)对每个样本，计算它作为oob样本的树对它的分类情况  
        2)以简单多数投票作为该样本的分类结果。  
        3)最后用误分类个数占样本总数的比率作为随机森林的oob误分率。  
        
## GBDT与lightGBM的区别
GBDT：CART，对损失函数求一阶导
XGBOOST:CART、线性分类器，对损失函数求二阶导，在代驾函数里加入了正则项，用于控制模型的复杂度。（包含了树的叶子节点个数、每个叶子节点上输出的score的L2模）  
的平方和。Shrinkage（缩减）。XGBOOST在进行完一次迭代后，会将叶子节点的权重乘上该系数。列抽样。特征有缺失值，XGBOOST可以自动学习出它的分裂方向。
XGBOOST的并行是在特征粒度上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）。XGBOOST在训练之前，预先对数据进行了排序，
然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。 

## XGBOOST和lightGBM的区别

## 决策树
决策树学习：特征选择、决策树的生成、决策树的修剪
## 决策树的增益
ID3:entropy计算增益
C4.5:信息增益比
CART:gini指数计算增益
XGBOOST：打分函数计算增益

## 特征重要性
### XGBOOST 
1 使用特征在所有树中作为划分属性的次数
2 使用特征在作为划分属性时Loss平均的损失量
3 使用特征在作为划分属性时对样本的覆盖度
如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，留下对训练数据有足够分类能力的特征。
