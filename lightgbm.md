# lightGBM
lightgbm的特点：
(1) Histogram:将特征离散化为离散特征，并放在不同的bin中。降低内存的占用。
(2) 直方图做差加速：父结点的直方图-子结点的直方图得到另一个子结点的直方图，加速程序的运行
(3) 带深度限制的leaf-wise的叶子生长策略。高效、精度高。
(4) 直接支持类别特征
(5) 直接支持高效并行。特征并行和数据并行。

# XGBOOST
损失函数加入了正则化项
XGBOOST是很多CART回归树的集成
XGBOOST的算法思想：1.对所有特征都按照特征的数值进行预排序
2.遍历分割点，找到特征上的最好分割点
3.将数据分裂成左右子节点

## 优点 
1.利用二阶梯度划分节点，精度高
2.利用局部近似算法对分裂节点的贪心算法优化，速度快
3.损失函数加入了L1/L2,提高了模型的鲁棒性
4.提供并行计算能力，主要是在树节点求不同的候选的分裂点的Gain Infomation
5.Tree Shrinkage，column subsampling等不同的处理细节。
## 缺点
1.需要pre-sorted，这个会耗掉很多的内存空间
2.数据分割点上，由于XGB对不同的数据特征使用pre-sorted算法而不同特征其排序顺序是不同的，所以分裂时需要对每个特征单独做依次分割，遍历次数为#data * #features来将数据分裂到左右子节点上。
3.尽管使用了局部近似计算，但是处理粒度还是太细了
4.由于pre-sorted处理数据，在寻找特征分裂点时（level-wise），会产生大量的cache随机访问。

# GBDT
GBDT 分类和回归的基学习器都是CART回归树，因为是残差拟合的。
分类问题：指数损失函数、对数损失函数
回归问题：均方差损失函数、绝对损失函数。
        分位数、Huber损失函数，可以增加回归问题的健壮性，可以减少异常点对损失函数的影响。

# XGBOOST和GBDT的区别与联系  https://blog.csdn.net/qq_22238533/article/details/79477547
区别： 
1.xgboost和GBDT的一个区别在于目标函数上。 
在xgboost中，损失函数+正则项。 
GBDT中，只有损失函数。 
2.xgboost中利用二阶导数的信息，而GBDT只利用了一阶导数。 
3.xgboost在建树的时候利用的准则来源于目标函数推导，而GBDT建树利用的是启发式准则。（这一点，我个人认为是xgboost牛B的所在，也是为啥要费劲二阶泰勒展开） 
4.xgboost中可以自动处理空缺值，自动学习空缺值的分裂方向，GBDT(sklearn版本）不允许包含空缺值。 
5.其他若干工程实现上的不同（这个由于本文没有涉及就不说了）

联系： 
1.xgboost和GBDT的学习过程都是一样的，都是基于Boosting的思想，先学习前n-1个学习器，然后基于前n-1个学习器学习第n个学习器。(Boosting) 
2.建树过程都利用了损失函数的导数信息(Gradient),只是大家利用的方式不一样而已。 
3.都使用了学习率来进行Shrinkage，从前面我们能看到不管是GBDT还是xgboost，我们都会利用学习率对拟合结果做缩减以减少过拟合的风险。

## 常见的指标metrics
l1 绝对值损失
l2 均方损失
MAPE mean_absolute_percentage_error
MAP   mean_avdrage_precision
auc

## XGBOOST是如何计算特征重要性的？
在单个决策树中通过每个属性分裂点改进性能度量的量来计算属性重要性，由节点负责加权和记录次数。一个属性对分裂点改进性能度量越大（越靠近根节点），权值越大；被越多提升树所选择，属性越重要。
性能度量可以是选择分裂节点的Gini纯度，也可以是其他度量函数。
最终将一个属性在所有提升树中的结果进行加权求和后然后平均，得到重要性得分。



binary_loss log loss
cross_entropy
